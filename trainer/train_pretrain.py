import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import argparse
import time
import warnings
import torch
import torch.distributed as dist
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler

from model import MiniGPTForCausalLM, MiniGPTConfig
from dataset.lm_dataset import PretrainDataset
from trainer.trainer_utils import *


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniGPT Pretraining")

    # ========== Basic Training Arguments ==========
    parser.add_argument("--save_dir", type=str, default="out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--save_weight", default="pretrain", type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å")
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®1è½®zeroæˆ–2-6è½®å……åˆ†è®­ç»ƒï¼‰")
    parser.add_argument("--batch_size", type=int, default=32, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="initial learning rate")

    # ========== Hardware Arguments ==========
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")

    # ========== Training Policy Arguments ==========
    parser.add_argument("--accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=100, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš”")

    # ========== Model Architecture Arguments ==========
    parser.add_argument("--hidden_size", default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_hidden_layers", default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument("--max_seq_len", default=512, type=int, help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦")
    parser.add_argument("--use_moe", default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")

    # ========== Data & Checkpoints Arguments ==========
    parser.add_argument("--data_path", type=str, default="dataset/pretrain_hq.jsonl", help="é¢„è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--from_weight", default="none", type=str, help="åŸºäºå“ªä¸ªæƒé‡è®­ç»ƒï¼Œä¸ºnoneåˆ™ä»å¤´å¼€å§‹")
    parser.add_argument("--from_resume", default=0, type=int, choices=[0, 1], help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")

    # ========== Experment Tracking Arguments ==========
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument("--wandb_project", type=str, default="MiniGPT-Pretrain", help="wandbé¡¹ç›®å")

    # parsing all arguments
    args = parser.parse_args()


    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    """
    ğŸ“š åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–çŸ¥è¯†ç‚¹ï¼š
    - local_rank: å½“å‰è¿›ç¨‹åœ¨æœ¬æœºä¸Šçš„GPUç¼–å·
    - éšæœºç§å­: ç¡®ä¿ä¸åŒè¿›ç¨‹æœ‰ä¸åŒä½†å¯å¤ç°çš„éšæœºåºåˆ—
    - è¿™æ ·æ—¢ä¿è¯äº†éšæœºæ€§ï¼Œåˆä¿è¯äº†å¯å¤ç°æ€§
    """
    local_rank = init_distributed_mode()
    if dist.is_initialized():
        args.device = f"cuda:{local_rank}"  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨å¯¹åº”çš„GPU

    # ğŸ“š éšæœºç§å­è®¾ç½®çŸ¥è¯†ç‚¹
    # ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ç§å­ï¼Œé¿å…æ•°æ®é‡‡æ ·å®Œå…¨ç›¸åŒ
    # 42æ˜¯åŸºç¡€ç§å­ï¼Œæ¯ä¸ªè¿›ç¨‹åŠ ä¸Šè‡ªå·±çš„rankä¿è¯ä¸åŒ
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))

    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ç‚¹ ==========
    """
    ğŸ“š æ¨¡å‹é…ç½®å’Œæ£€æŸ¥ç‚¹ç®¡ç†ï¼š
    - åˆ›å»ºä¿å­˜ç›®å½•
    - æ„å»ºæ¨¡å‹é…ç½®å¯¹è±¡
    - å°è¯•åŠ è½½æ–­ç‚¹ç»­è®­æ•°æ®
    """
    os.makedirs(args.save_dir, exist_ok=True)  # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨

    # åˆ›å»ºMiniMindæ¨¡å‹é…ç½®
    lm_config = MiniGPTConfig(
        hidden_size=args.hidden_size, 
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe)
    )

    # ğŸ“š æ–­ç‚¹ç»­è®­çŸ¥è¯†ç‚¹
    # å¦‚æœå¼€å¯äº†æ–­ç‚¹ç»­è®­ï¼Œå°è¯•åŠ è½½ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€
    ckp_data = (
        lm_checkpoint(lm_config, weight=args.save_weight, save_dir="checkpoints")
        if args.from_resume == 1
        else None
    )

    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    """
    ğŸ“š æ··åˆç²¾åº¦è®­ç»ƒçŸ¥è¯†ç‚¹ï¼š
    - bfloat16: Googleå¼€å‘ï¼Œæ•°å€¼èŒƒå›´å¤§ï¼Œæ›´ç¨³å®š
    - float16: æ ‡å‡†åŠç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜ä½†å¯èƒ½æº¢å‡º
    - autocast: è‡ªåŠ¨é€‰æ‹©ç²¾åº¦ï¼Œå…³é”®è¿ç®—ç”¨float32
    """
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16

    # ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†å™¨çŸ¥è¯†ç‚¹
    # CPUä¸æ”¯æŒautocastï¼Œä½¿ç”¨nullcontextä½œä¸ºç©ºæ“ä½œ
    autocast_ctx = (
        nullcontext() if device_type == "cpu" else torch.amp.autocast("cuda", dtype=dtype)
    )

    # ========== 4. é…ç½®WandBå®éªŒè·Ÿè¸ª ==========
    """
    ğŸ“š å®éªŒè·Ÿè¸ªç³»ç»ŸçŸ¥è¯†ç‚¹ï¼š
    - WandB: å®éªŒç®¡ç†å¹³å°ï¼Œè®°å½•è®­ç»ƒè¿‡ç¨‹
    - SwanLab: å›½äº§æ›¿ä»£æ–¹æ¡ˆ
    - æ”¯æŒæ–­ç‚¹ç»­è®­æ—¶æ¢å¤åˆ°åŒä¸€ä¸ªå®éªŒ
    """
    wandb = None
    if args.use_wandb and is_main_process():
        # ä½¿ç”¨SwanLabä½œä¸ºWandBçš„æ›¿ä»£
        import swanlab as wandb

        # ğŸ“š å®éªŒæ¢å¤çŸ¥è¯†ç‚¹
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹æ•°æ®ï¼Œè·å–ä¹‹å‰çš„wandb_idæ¥æ¢å¤å®éªŒ
        wandb_id = ckp_data.get("wandb_id") if ckp_data else None
        resume = "must" if wandb_id else None  # å¿…é¡»æ¢å¤åˆ°æŒ‡å®šå®éªŒ

        # æ„å»ºå®éªŒåç§°ï¼ŒåŒ…å«å…³é”®è¶…å‚æ•°
        wandb_run_name = f"MiniGPT-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        wandb.init(
            project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume
        )

    # ========== 5. å®šä¹‰æ¨¡å‹ã€æ•°æ®ã€ä¼˜åŒ–å™¨ ==========
    """
    ğŸ“š è®­ç»ƒç»„ä»¶åˆå§‹åŒ–ï¼š
    - æ¨¡å‹: æ ¹æ®é…ç½®åˆ›å»ºMiniGPTæ¨¡å‹
    - æ•°æ®é›†: åŠ è½½é¢„è®­ç»ƒæ•°æ®
    - é‡‡æ ·å™¨: åˆ†å¸ƒå¼è®­ç»ƒçš„æ•°æ®åˆ†é…
    - ä¼˜åŒ–å™¨: AdamWä¼˜åŒ–å™¨
    - ç¼©æ”¾å™¨: æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾
    """
    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)

    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)

    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None

    scaler = torch.amp.GradScaler("cuda", enabled=(args.dtype == "float16"))

    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    start_epoch, start_step = 0, 0
    if ckp_data:
        # æ¢å¤æ¨¡å‹å‚æ•°
        model.load_state_dict(ckp_data["model"])
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡ã€æ–¹å·®ä¼°è®¡ç­‰ï¼‰
        optimizer.load_state_dict(ckp_data["optimizer"])
        # æ¢å¤æ¢¯åº¦ç¼©æ”¾å™¨çŠ¶æ€
        scaler.load_state_dict(ckp_data["scaler"])
        # æ¢å¤è®­ç»ƒè¿›åº¦
        start_epoch = ckp_data["epoch"]
        start_step = ckp_data.get("step", 0)

    if dist.is_initialized():
        # ğŸ“š RoPEä½ç½®ç¼–ç ç‰¹æ®Šå¤„ç†
        # freqs_cos, freqs_sinæ˜¯ä½ç½®ç¼–ç ç¼“å­˜ï¼Œä¸éœ€è¦æ¢¯åº¦åŒæ­¥
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])

    for epoch in range(start_epoch, args.epochs):
        # ğŸ“š åˆ†å¸ƒå¼é‡‡æ ·å™¨epochè®¾ç½®
        # æ¯ä¸ªepochè®¾ç½®ä¸åŒçš„éšæœºç§å­ï¼Œç¡®ä¿æ•°æ®é¡ºåºéšæœºåŒ–
        if train_sampler:
            train_sampler.set_epoch(epoch)

        # ğŸ“š æ–­ç‚¹ç»­è®­é€»è¾‘
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            # ä½¿ç”¨è·³æ‰¹é‡‡æ ·å™¨ï¼Œè·³è¿‡å·²è®­ç»ƒçš„æ•°æ®
            batch_sampler = SkipBatchSampler(
                train_sampler or range(len(train_ds)), args.batch_size, start_step + 1
            )
            loader = DataLoader(
                train_ds,
                batch_sampler=batch_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            Logger(
                f"Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹"
            )
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(
                train_ds,
                batch_size=args.batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            train_epoch(epoch, loader, len(loader), 0, wandb)